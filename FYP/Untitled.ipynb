{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import process_tweet, lookup  # For pre processing of tweets\n",
    "import pdb\n",
    "import math\n",
    "import nltk\n",
    "import string\n",
    "import re   # Regular expression\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "import csv # to read the csv file of stop words\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "import sklearn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SKLEARN: 0.19.1\n",
      "NUMPY: 1.14.3\n",
      "NLTK: 3.3\n",
      "SCIPY 1.1.0\n",
      "pip install -U scikit-learn==0.19.1\n",
      "pip install numpy==1.14.3\n",
      "pip install nltk==3.3\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"SKLEARN:\",sklearn.__version__)\n",
    "print(\"NUMPY:\",np.__version__)\n",
    "print(\"NLTK:\",nltk.__version__)\n",
    "import scipy\n",
    "print(\"SCIPY\",scipy.__version__)\n",
    "\n",
    "req = '''pip install -U scikit-learn==0.19.1\n",
    "pip install numpy==1.14.3\n",
    "pip install nltk==3.3\n",
    "'''\n",
    "print(req)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "Stop_Words=[]\n",
    "with open('StopWords.csv', 'r') as file:  # Read the stop words from file and store them in the List\n",
    "    reader = csv.reader(file)\n",
    "    i=0\n",
    "    for row in reader:\n",
    "        Stop_Words.insert(i,row)\n",
    "        i+=1\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function to pre process the tweets\n",
    "def Pre_Process(Tweets):\n",
    "    \n",
    "    # remove stock market tickers like $GE\n",
    "\n",
    "    #Tweets = re.sub(r'\\$\\w*', '', Tweets)\n",
    "\n",
    "    # remove old style retweet text \"RT\"\n",
    "\n",
    "     #  tweet = re.sub(r'^RT[\\s]+', '', tweet)\n",
    "\n",
    "    # remove hyperlinks\n",
    "\n",
    " #   tweet = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', tweet)\n",
    "\n",
    "    # remove hashtags\n",
    "\n",
    "    # only removing the hash # sign from the word\n",
    "\n",
    "  #  tweet = re.sub(r'#', '', tweet)\n",
    "    \n",
    "    # tokenize tweets\n",
    "    Tokenized_Tweets=[[]]\n",
    "    tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True,reduce_len=True) # Case-Folding, Tokenization\n",
    "    for i in range(len(Tweets)-1):\n",
    "        tweet_tokens = tokenizer.tokenize(Tweets[i])\n",
    "        Tokenized_Tweets.insert(i,tweet_tokens)\n",
    "    #print(\"Number of Tokenized Tweets = \"+str(len(Tokenized_Tweets)))\n",
    "    #print(Tokenized_Tweets)\n",
    "    #print(\"len(Tokenized)\",len(Tokenized_Tweets))\n",
    "    cleaned_tweets = []\n",
    "    for i in range (len(Tokenized_Tweets)-1): # For all tweets\n",
    "        inner=[]\n",
    "        for k in range(0,len(Tokenized_Tweets[i])): # For all the words of one tweet\n",
    "            check=0\n",
    "            for j in range(len(Stop_Words)):        # Iterate for all the words of one tweet in Stop Words List\n",
    "                if(Tokenized_Tweets[i][k]==Stop_Words[j][0]): #check if the words of one tweet are in Stop Words List\n",
    "                    check=1 \n",
    "                    break\n",
    "            if Tokenized_Tweets[i][k] in string.punctuation:  # Removes punctuation\n",
    "                check=1\n",
    "            if not check:        \n",
    "                inner.append(Tokenized_Tweets[i][k])\n",
    "        cleaned_tweets.append(inner)\n",
    "    return cleaned_tweets\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the tokenized tweets back to string\n",
    "def Remove_Stop_Words(x):# Combine tokenzied words to strings again after removing stop words\n",
    "\n",
    "    Tweets=[]\n",
    "    b=''\n",
    "    i=0\n",
    "    for tweet in x:\n",
    "        for word in tweet:\n",
    "            b=b+word\n",
    "            b=b+' '\n",
    "        Tweets.insert(i,b)\n",
    "        i+=1\n",
    "    return Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data  of tweets\n",
    "\n",
    "import xlrd       # For reading excel file\n",
    "from array import *   \n",
    "loc=(\"C:\\\\Users\\\\Mr.Wick\\\\Videos\\\\Fall2020\\\\FYP\\\\dataset.xlsx\") # Loading dataset \n",
    "wb = xlrd.open_workbook(loc)   \n",
    "sheet = wb.sheet_by_index(0)\n",
    "\n",
    "\n",
    "\n",
    "TWEETS=[]   # a list to store tweets\n",
    "labels=[]   # A list to store labels\n",
    "\n",
    "rows=sheet.nrows\n",
    "\n",
    "for i in range(1,rows):\n",
    "    labels.insert(i,sheet.cell_value(i,1))\n",
    "    TWEETS.insert(i,sheet.cell_value(i,0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "karkhanon dhuwan mahol aloodah kerta lahore mazeed 7 elaaqay seal ker diye gaye \n"
     ]
    }
   ],
   "source": [
    "cleaned_tweets=Pre_Process(TWEETS)\n",
    "t= Remove_Stop_Words(cleaned_tweets)\n",
    "print(t[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'tfidf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-72-c5b212d22d94>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtfidfconverter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtfidf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"tfidf.pickle\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"wb\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'tfidf' is not defined"
     ]
    }
   ],
   "source": [
    "#Converting the tweets to the TF-IDFs\n",
    "tfidfconverter = TfidfVectorizer()\n",
    "X = tfidfconverter.fit(t)\n",
    "print(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(X, open(\"tfidf.pickle\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tfidfconverter.transform(X).toarray()\n",
    "print(len(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4999, 5602)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the data into training and testing \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, labels, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[567  35]\n",
      " [  0 398]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          N       1.00      0.94      0.97       602\n",
      "          R       0.92      1.00      0.96       398\n",
      "\n",
      "avg / total       0.97      0.96      0.97      1000\n",
      "\n",
      "0.965\n"
     ]
    }
   ],
   "source": [
    "# Neural Network \n",
    "classifier3=MLPClassifier(alpha=1, max_iter=2000)\n",
    "classifier3.fit(X_train, y_train)\n",
    "Y_pred = classifier3.predict(X_test)\n",
    "print(confusion_matrix(y_test,Y_pred))\n",
    "print(classification_report(y_test,Y_pred))\n",
    "\n",
    "print(accuracy_score(y_test, Y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "After Pre_Processing [['log', 'kehtey', 'taqreer', 'kia', 'hoga', 'hazrat', 'jafer', 'tayyar', 'taqreer', 'jis', 'najashi', 'dil', 'badal', 'diya']]\n",
      "Converting back to string ['log kehtey taqreer kia hoga hazrat jafer tayyar taqreer jis najashi dil badal diya ']\n",
      "tf-idf [[0. 0. 0. ... 0. 0. 0.]]\n",
      "['R']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "a=['log kehtey hein ke taqreer se kia hoga, hazrat jafer tayyar ne bhi taqreer ki thi, jis ne najashi ka dil badal diya tha',[]]\n",
    "print(len(a))\n",
    "tfidfconverter = pickle.load(open('tfidf.pickle', 'rb'))\n",
    "a=Pre_Process(a)\n",
    "print(\"After Pre_Processing\", a)\n",
    "a=Remove_Stop_Words(a)\n",
    "print(\"Converting back to string\",a)\n",
    "x = tfidfconverter.transform(a).toarray()\n",
    "print(\"tf-idf\",x)\n",
    "y_pred = classifier3.predict(x)\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save neural network model\n",
    "import pickle\n",
    "filename='NeuralNetworks.sav'\n",
    "pickle.dump(classifier3, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.965\n"
     ]
    }
   ],
   "source": [
    "# load the model from disk\n",
    "loaded_model = pickle.load(open(filename, 'rb'))\n",
    "result = loaded_model.score(X_test, y_test)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.19.1\n"
     ]
    }
   ],
   "source": [
    "print(sklearn.__version__)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
