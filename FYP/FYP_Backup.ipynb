{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import process_tweet, lookup  # For pre processing of tweets\n",
    "import pdb\n",
    "import math\n",
    "import nltk\n",
    "import string\n",
    "import re   # Regular expression\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "import csv # to read the csv file of stop words\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Naive Bayes Algorithm implementation\n",
    "class Naive_Bayes():\n",
    "    \n",
    "    Stop_Words=[]\n",
    "    def __init__(self): # Default constructor\n",
    "        self.Stop_Words=[]\n",
    "    #Read stop words and store them in the dictionary\n",
    "    def Read_Stop_Words(self):\n",
    "        \n",
    "        with open('StopWords.csv', 'r') as file:  # Read the stop words from file and store them in the List\n",
    "            reader = csv.reader(file)\n",
    "            i=0\n",
    "            for row in reader:\n",
    "                Stop_Words.insert(i,row)\n",
    "                i+=1\n",
    "        file.close()\n",
    "        \n",
    "    \n",
    "    # Function to Pre_Process the tweets\n",
    "    # A function to pre process the tweets\n",
    "    def Pre_Process(self,Tweets):\n",
    "    \n",
    "    # remove stock market tickers like $GE\n",
    "\n",
    "    #tweet = re.sub(r'\\$\\w*', '', tweet)\n",
    "\n",
    "    # remove old style retweet text \"RT\"\n",
    "\n",
    "     #  tweet = re.sub(r'^RT[\\s]+', '', tweet)\n",
    "\n",
    "    # remove hyperlinks\n",
    "\n",
    " #   tweet = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', tweet)\n",
    "\n",
    "    # remove hashtags\n",
    "\n",
    "    # only removing the hash # sign from the word\n",
    "\n",
    "  #  tweet = re.sub(r'#', '', tweet)\n",
    "    \n",
    "    # tokenize tweets\n",
    "        Tokenized_Tweets=[[]]\n",
    "        tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True,reduce_len=True) # Case-Folding, Tokenization\n",
    "        for i in range(len(Tweets)-1):\n",
    "            tweet_tokens = tokenizer.tokenize(Tweets[i])\n",
    "            Tokenized_Tweets.insert(i,tweet_tokens)\n",
    "        #print(\"Number of Tokenized Tweets = \"+str(len(Tokenized_Tweets)))\n",
    "        #print(Tokenized_Tweets)\n",
    "        #print(\"len(Tokenized)\",len(Tokenized_Tweets))\n",
    "        cleaned_tweets = []\n",
    "        for i in range (len(Tokenized_Tweets)): # For all tweets\n",
    "            inner=[]\n",
    "            for k in range(0,len(Tokenized_Tweets[i])): # For all the words of one tweet\n",
    "                check=0\n",
    "                for j in range(len(self.Stop_Words)):        # Iterate for all the words of one tweet in Stop Words List\n",
    "                    if(Tokenized_Tweets[i][k]==self.Stop_Words[j][0]): #check if the words of one tweet are in Stop Words List\n",
    "                        check=1 \n",
    "                        break\n",
    "                if Tokenized_Tweets[i][k] in string.punctuation:  # Removes punctuation\n",
    "                    check=1\n",
    "                if not check:        \n",
    "                    inner.append(Tokenized_Tweets[i][k])\n",
    "            cleaned_tweets.append(inner)\n",
    "        return cleaned_tweets\n",
    "    \n",
    "    # function to load and clean the data\n",
    "    def Load_Data(self):\n",
    "        loc=(\"C:\\\\Users\\\\Mr.Wick\\\\Videos\\\\Fall2020\\\\FYP\\\\dataset.xlsx\") # Loading dataset \n",
    "        wb = xlrd.open_workbook(loc)   \n",
    "        sheet = wb.sheet_by_index(0)\n",
    "\n",
    "\n",
    "\n",
    "        TWEETS=[]   # a list to store tweets\n",
    "        labels=[]   # A list to store labels\n",
    "\n",
    "        rows=sheet.nrows\n",
    "\n",
    "        for i in range(1,rows):\n",
    "            labels.insert(i,sheet.cell_value(i,1))\n",
    "            TWEETS.insert(i,sheet.cell_value(i,0))\n",
    "        Log_Prior=0\n",
    "        Racial=0\n",
    "        Non_Racial=0\n",
    "        for i in range(len(labels)-1):\n",
    "            if(labels[i]=='R'):\n",
    "                Racial+=1\n",
    "            else:\n",
    "                Non_Racial+=1\n",
    "        print(\"No. of Racial Tweets : \",Racial)\n",
    "        print(\"No. of Non Racial Tweets : \",Non_Racial)\n",
    "        Log_Prior=Racial/Non_Racial\n",
    "        print(\"Log Prior = \", Log_Prior)\n",
    "    \n",
    "        cleaned_tweets=Pre_Process(TWEETS)\n",
    "        #print(cleaned_tweets)\n",
    "    def in_dictionary(self,word):\n",
    "            for i in range(len(Dictionary)-1):\n",
    "                if word==Dictionary[i][0]:\n",
    "                    return True\n",
    "            else:\n",
    "                return False\n",
    "    def Return_Index(self,word):\n",
    "            i=0\n",
    "            for i in range (len(Dictionary)-1):\n",
    "                if word==Dictionary[i][0]:\n",
    "                    return i\n",
    "    def BOW(self):\n",
    "        i=0\n",
    "        dic=[[]]\n",
    "        label_=Y_Train\n",
    "        positive=0\n",
    "        Negative=0\n",
    "        for tweet in X_Train:\n",
    "            for word in tweet:\n",
    "                if label_[i]=='R':\n",
    "                    tup=[word,'R']\n",
    "                    dic.insert(i,tup)\n",
    "                else:\n",
    "                    tup=[word,'N']\n",
    "                    dic.insert(i,tup)\n",
    "            i+=1\n",
    "        i=0\n",
    "        j=0\n",
    "\n",
    "        Dictionary=[[]]\n",
    "        \n",
    "        i=0\n",
    "        length=0\n",
    "\n",
    "        for i in range (len(dic)-1):\n",
    "            if (in_dictionary(dic[i][0])==True) and (dic[i][1]=='R'):\n",
    "                length=self.Return_Index(dic[i][0])\n",
    "                Dictionary[length][1]=Dictionary[length][1]+1\n",
    "            elif (in_dictionary(dic[i][0])==True) and (dic[i][1]=='N'):\n",
    "                length=self.Return_Index(dic[i][0])\n",
    "                Dictionary[length][2]=Dictionary[length][2]+1\n",
    "            else:\n",
    "                if (dic[i][1]=='N'):\n",
    "                    tuples=[dic[i][0],0,1]\n",
    "                else:\n",
    "                    tuples=[dic[i][0],1,0]\n",
    "                Dictionary.insert(len(Dictionary)-1,tuples)\n",
    "        #print(Dictionary)\n",
    "\n",
    "        i=0\n",
    "        No_unique_words=len(Dictionary)-1\n",
    "\n",
    "        Sum_positive_freq=0\n",
    "        Sum_negative_freq=0\n",
    "        for i in range(len(Dictionary)-1):\n",
    "            Sum_positive_freq=Sum_positive_freq+Dictionary[i][1]\n",
    "            Sum_negative_freq=Sum_negative_freq+Dictionary[i][2]\n",
    "        #print(Sum_positive_freq)\n",
    "        #print(Sum_negative_freq)\n",
    "        #print(No_unique_words)\n",
    "        # Calculating Probabilties of each unique word in each class\n",
    "\n",
    "        i=0\n",
    "        Dict_of_Probs=[[]]\n",
    "        for i in range(len(Dictionary)-1):\n",
    "    \n",
    "            pos_prob=(Dictionary[i][1]+1)/(Sum_positive_freq+No_unique_words)\n",
    "            neg_prob=(Dictionary[i][2]+1)/(Sum_negative_freq+No_unique_words)\n",
    "            Lambda=math.log(pos_prob/neg_prob)\n",
    "            row=[Dictionary[i][0],pos_prob,neg_prob,Lambda]\n",
    "            Dict_of_Probs.insert(i,row)\n",
    "    \n",
    "        #print(Dict_of_Probs)\n",
    "        def in_prob_dic(self,word):\n",
    "            for i in range(len(Dict_of_Probs)-1):\n",
    "                if (word==Dict_of_Probs[i][0]):\n",
    "                    return Dict_of_Probs[i][3]\n",
    "            return 0\n",
    "    #Function to calculate likelihoods\n",
    "    def Calculate_Likelihood(self,tweetss):\n",
    "        log_likelihood=0\n",
    "        likelihood=0\n",
    "        if(tweetss== []):\n",
    "            return 0\n",
    "        for i in range(len(tweetss)-1):\n",
    "            likelihood=in_prob_dic(tweetss[i])\n",
    "            log_likelihood=log_likelihood+likelihood\n",
    "            \n",
    "        log_likelihood=Log_Prior+log_likelihood\n",
    "        return log_likelihood\n",
    "    def Score(self,X_Test):\n",
    "        #def Evaluate(test_twee):\n",
    "        log_likelihood=0\n",
    "        likelihood=0\n",
    "    \n",
    "        positive_count=0 # to count the number of predicted correct tweets\n",
    "    \n",
    "        j=0\n",
    "        for tweetss in X_Test:\n",
    "            if(tweetss== []):\n",
    "                continue\n",
    "            for i in range(len(tweetss)-1):\n",
    "                likelihood=in_prob_dic(tweetss[i])\n",
    "                log_likelihood=log_likelihood+likelihood\n",
    "            log_likelihood=Log_Prior+log_likelihood\n",
    "\n",
    "            if (log_likelihood>0):\n",
    "                if(Y_Test[j]=='R'):\n",
    "                    positive_count+=1\n",
    "            elif (log_likelihood<0):\n",
    "                if(Y_Test[j]=='N'):\n",
    "                    positive_count+=1\n",
    "            else:\n",
    "                Print(\"Error in classifying the tweet\")\n",
    "            log_likelihood=0\n",
    "            likelihood=0\n",
    "            j+=1\n",
    "        score=positive_count/len(X_Test)\n",
    "        return score\n",
    "    def Predict(self, Text):\n",
    "        #cleaned_text=self.Pre_Process(Text)\n",
    "        if(Text==''):\n",
    "            print(\"Enter something to predict.\")\n",
    "            return \n",
    "        print(Text)\n",
    "        Prediction=self.Calculate_Likelihood(Text)\n",
    "        print(\"prediction=\",Prediction)\n",
    "        if(Prediction>0):\n",
    "            return 1\n",
    "        elif(Prediction ==0):\n",
    "            return -1\n",
    "        elif(Prediction<0):\n",
    "            return 0\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of Racial Tweets :  2040\n",
      "No. of Non Racial Tweets :  2958\n",
      "Log Prior =  0.6896551724137931\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-214-b632206202cb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mRead_Stop_Words\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLoad_Data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mBOW\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLoad_Data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m#obj.Pre_Process(Stop_Words)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-212-88c993c98865>\u001b[0m in \u001b[0;36mBOW\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    132\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0min_dictionary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdic\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mdic\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;34m'R'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    133\u001b[0m                 \u001b[0mlength\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mReturn_Index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdic\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 134\u001b[1;33m                 \u001b[0mDictionary\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlength\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mDictionary\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlength\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    135\u001b[0m             \u001b[1;32melif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0min_dictionary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdic\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mdic\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;34m'N'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    136\u001b[0m                 \u001b[0mlength\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mReturn_Index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdic\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "obj=Naive_Bayes()\n",
    "obj.Read_Stop_Words()\n",
    "obj.Load_Data()\n",
    "obj.BOW()\n",
    "obj.Load_Data()\n",
    "#obj.Pre_Process(Stop_Words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test train split\n",
    "X_Train, X_Test, Y_Train, Y_Test= train_test_split(cleaned_tweets,labels,test_size=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(obj.Calculate_Likelihood(X_Test[9]))\n",
    "print(X_Test[9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus=['Aaj kal nokri hasil karney ke liye bari sifarish ki zarurat perti hai']\n",
    "c=Naive_Bayes.Pre_Process(corpus)\n",
    "print(c)\n",
    "s=obj.Predict(c)\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len of dic=  34763\n",
      "i =  3999\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "i=0\n",
    "j=0\n",
    "\n",
    "Dictionary=[[]]\n",
    "def in_dictionary(word):\n",
    "    for i in range(len(Dictionary)-1):\n",
    "        if word==Dictionary[i][0]:\n",
    "            return True\n",
    "    else:\n",
    "        return False\n",
    "i=0\n",
    "length=0\n",
    "\n",
    "def Return_Index(word):\n",
    "    i=0\n",
    "    for i in range (len(Dictionary)-1):\n",
    "        if word==Dictionary[i][0]:\n",
    "            return i\n",
    "\n",
    "for i in range (len(dic)-1):\n",
    "    if (in_dictionary(dic[i][0])==True) and (dic[i][1]=='R'):\n",
    "        length=Return_Index(dic[i][0])\n",
    "        Dictionary[length][1]=Dictionary[length][1]+1\n",
    "    elif (in_dictionary(dic[i][0])==True) and (dic[i][1]=='N'):\n",
    "        length=Return_Index(dic[i][0])\n",
    "        Dictionary[length][2]=Dictionary[length][2]+1\n",
    "    else:\n",
    "        if (dic[i][1]=='N'):\n",
    "            tuples=[dic[i][0],0,1]\n",
    "        else:\n",
    "            tuples=[dic[i][0],1,0]\n",
    "        Dictionary.insert(len(Dictionary)-1,tuples)\n",
    "#print(Dictionary)\n",
    "\n",
    "i=0\n",
    "No_unique_words=len(Dictionary)-1\n",
    "\n",
    "Sum_positive_freq=0\n",
    "Sum_negative_freq=0\n",
    "for i in range(len(Dictionary)-1):\n",
    "    Sum_positive_freq=Sum_positive_freq+Dictionary[i][1]\n",
    "    Sum_negative_freq=Sum_negative_freq+Dictionary[i][2]\n",
    "#print(Sum_positive_freq)\n",
    "#print(Sum_negative_freq)\n",
    "#print(No_unique_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.713\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy =\", Score())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Predict_NB(tweet):\n",
    "    cleaned=Pre_Process(tweet)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Remove_Stop_Words(x):# Combine tokenzied words to strings again after removing stop words\n",
    "\n",
    "    Tweets=[]\n",
    "    a=' '\n",
    "    i=0\n",
    "    for tweet in x:\n",
    "        \n",
    "\n",
    "        for word in tweet:\n",
    "            a=a+word\n",
    "            a=a+' '\n",
    "        Tweets.insert(i,a)\n",
    "        i+=1\n",
    "    return Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " karkhanon dhuwan mahol aloodah kerta lahore mazeed 7 elaaqay seal ker diye gaye \n"
     ]
    }
   ],
   "source": [
    "t= Remove_Stop_Words(cleaned_tweets)\n",
    "print(t[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4999"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Labels=[]\n",
    "i=0\n",
    "for label in (labels):\n",
    "    if(label=='R'):\n",
    "        Labels.insert(i,1)\n",
    "    else:\n",
    "        Labels.insert(i,0)\n",
    "    i+=1\n",
    "len(Labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4999\n"
     ]
    }
   ],
   "source": [
    "#Converting the tweets to the TF-IDFs\n",
    "tfidfconverter = TfidfVectorizer()\n",
    "X = tfidfconverter.fit_transform(t).toarray()\n",
    "print(len(X))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. ... 0. 0. 0.]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5602"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(X[1])\n",
    "len(X[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the data into training and testing \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, labels, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5602,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets=['Pakistan zindabad']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import make_moons, make_circles, make_classification\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "# Random Forest Classifier\n",
    "classifier2 = RandomForestClassifier(n_estimators=1000, random_state=0)\n",
    "classifier2.fit(X_train, y_train)\n",
    "y_pred = classifier2.predict(X_test)\n",
    "\n",
    "#Save neural network model\n",
    "#import pickle\n",
    "#filename='RandomForest.sav'\n",
    "#pickle.dump(classifier3, open(filename, 'wb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[561   0   1]\n",
      " [  1   0   0]\n",
      " [  0   0 437]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          N       1.00      1.00      1.00       562\n",
      "         N        0.00      0.00      0.00         1\n",
      "          R       1.00      1.00      1.00       437\n",
      "\n",
      "avg / total       1.00      1.00      1.00      1000\n",
      "\n",
      "0.998\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Anaconda3_64\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(y_test,y_pred))\n",
    "print(classification_report(y_test,y_pred))\n",
    "print(accuracy_score(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['N']\n"
     ]
    }
   ],
   "source": [
    "a=['kutia tu ne ghar ki safai thek se kyun nahi ki?']\n",
    "#tfidfconverter = TfidfVectorizer()\n",
    "a=Pre_Process(a)\n",
    "a=Remove_Stop_Words(a)\n",
    "\n",
    "x = tfidfconverter.transform(a).toarray()\n",
    "y_pred = classifier2.predict(x)\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[562   0   0]\n",
      " [  1   0   0]\n",
      " [ 11   0 426]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          N       0.98      1.00      0.99       562\n",
      "         N        0.00      0.00      0.00         1\n",
      "          R       1.00      0.97      0.99       437\n",
      "\n",
      "avg / total       0.99      0.99      0.99      1000\n",
      "\n",
      "0.988\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Anaconda3_64\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "#Naive Bayes Classifier\n",
    "Classifier1=GaussianNB()\n",
    "Classifier1.fit(X_train, y_train)\n",
    "y_pred = Classifier1.predict(X_test)\n",
    "print(confusion_matrix(y_test,y_pred))\n",
    "print(classification_report(y_test,y_pred))\n",
    "print(accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'classifier1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-69-8c7a5b0b1de8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtfidfconverter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclassifier1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'classifier1' is not defined"
     ]
    }
   ],
   "source": [
    "a=['kutia tu ne ghar ki safai thek se kyun nahi ki?']\n",
    "#tfidfconverter = TfidfVectorizer()\n",
    "a=Pre_Process(a)\n",
    "a=Remove_Stop_Words(a)\n",
    "\n",
    "x = tfidfconverter.transform(a).toarray()\n",
    "y_pred = classifier1.predict(x)\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[614   1   1]\n",
      " [  0   0   0]\n",
      " [  0   0 384]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          N       1.00      1.00      1.00       616\n",
      "         N        0.00      0.00      0.00         0\n",
      "          R       1.00      1.00      1.00       384\n",
      "\n",
      "avg / total       1.00      1.00      1.00      1000\n",
      "\n",
      "0.998\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Anaconda3_64\\lib\\site-packages\\sklearn\\metrics\\classification.py:1137: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "# Random Forest Classifier\n",
    "classifier2 = RandomForestClassifier(n_estimators=1000, random_state=0)\n",
    "classifier2.fit(X_train, y_train)\n",
    "y_pred = classifier2.predict(X_test)\n",
    "print(confusion_matrix(y_test,y_pred))\n",
    "print(classification_report(y_test,y_pred))\n",
    "print(accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[559   0   3]\n",
      " [  0   0   1]\n",
      " [  0   0 437]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          N       1.00      0.99      1.00       562\n",
      "         N        0.00      0.00      0.00         1\n",
      "          R       0.99      1.00      1.00       437\n",
      "\n",
      "avg / total       1.00      1.00      1.00      1000\n",
      "\n",
      "0.996\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Anaconda3_64\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "# Neural Network \n",
    "import sklearn\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "classifier3=MLPClassifier(alpha=1, max_iter=2000)\n",
    "classifier3.fit(X_train, y_train)\n",
    "Y_pred = classifier3.predict(X_test)\n",
    "print(confusion_matrix(y_test,Y_pred))\n",
    "print(classification_report(y_test,Y_pred))\n",
    "print(accuracy_score(y_test, Y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save neural network model\n",
    "import pickle\n",
    "filename='finalized.sav'\n",
    "pickle.dump(classifier3, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.925\n"
     ]
    }
   ],
   "source": [
    "# load the model from disk\n",
    "loaded_model = pickle.load(open(filename, 'rb'))\n",
    "result = loaded_model.score(X_test, y_test)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[578  38]\n",
      " [  0 384]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          N       1.00      0.94      0.97       616\n",
      "          R       0.91      1.00      0.95       384\n",
      "\n",
      "avg / total       0.97      0.96      0.96      1000\n",
      "\n",
      "0.962\n"
     ]
    }
   ],
   "source": [
    "#RBF SVM\n",
    "classifier4=SVC(gamma=2, C=1)\n",
    "classifier4.fit(X_train, y_train)\n",
    "y_pred = classifier4.predict(X_test)\n",
    "print(confusion_matrix(y_test,y_pred))\n",
    "print(classification_report(y_test,y_pred))\n",
    "print(accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[616   0]\n",
      " [  0 384]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          N       1.00      1.00      1.00       616\n",
      "          R       1.00      1.00      1.00       384\n",
      "\n",
      "avg / total       1.00      1.00      1.00      1000\n",
      "\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "# Ada Bost\n",
    "classifier5=AdaBoostClassifier()\n",
    "classifier5.fit(X_train, y_train)\n",
    "y_pred = classifier5.predict(X_test)\n",
    "print(confusion_matrix(y_test,y_pred))\n",
    "print(classification_report(y_test,y_pred))\n",
    "print(accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'QuadraticDiscriminantAnalysis' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-52aa34c575a9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mclassifier6\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mQuadraticDiscriminantAnalysis\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mclassifier6\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclassifier6\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfusion_matrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclassification_report\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'QuadraticDiscriminantAnalysis' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
